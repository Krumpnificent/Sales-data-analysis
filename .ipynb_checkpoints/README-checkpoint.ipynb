{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA ANALYSIS ON ALL THREE MAJOR BRANCHES OF  COMPANY XYZ ACROSS THE COUNTRY.\n",
    "\n",
    "\n",
    "## PROJECT DESCRIPTION:\n",
    "\n",
    "    The kernel of this project is to run a deep data analysis on Company XYZ which owns a supermarket chain across the country, with each major branch located in 3 major cities namely Abuja, Lagos and Port Harcourt. \n",
    "    \n",
    "    the analysis is based on recorded 3-month sales information. this analysis is to anable XYZ understand its growth trajectory, identify productivity loop holes to abate growth bottlenecks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Steps\n",
    "Explain in detail, the project steps and overwiew of different tasks completed here.\n",
    "\n",
    "### LOADING DATA SETS\n",
    "- I imported *pandas* as pd and *datetime* as dt to enable me work with pandas DataFrame and Datetime data type.\n",
    "\n",
    "- The goal is to run analysis on all the recoreds at once, so i opend the datasets download from the forked project from github as instructed and assing each of them to a variable. \n",
    "\n",
    "- I imported other variables such as numpy to handle mathematical evaluation, seaborn which is a visualization library, matplotlib.pyplot bacause seaborn is based on it, and warning to ignore warning prompts.\n",
    "\n",
    "- I started the main work by first concatenating the variable in which the three records from the the major branches of XYZ were asigned to from the very start into one variable (**main_list**).\n",
    "\n",
    "\n",
    "### Data exploration\n",
    "\n",
    "\n",
    "- I ran main_list.head() to see the first 4 rows of the compiled data sets, starting from the top, to get an idea of what my dataset looks lik.\n",
    "\n",
    "\n",
    "- I ran main_list.shape to see the shape of my dataset and the result was (1000, 17) meaning 1000 rows and 17 columns.\n",
    "\n",
    "\n",
    "- i assigned mainlist.columns to a variable **column_list** to get a nice dataframe view of the list of columns available in mainlist.\n",
    "\n",
    "\n",
    "- moving on, i used thwe main_list.describe() methond to run a complete mathematical estimate on the numerical values of the given records. these estimates are the count, mean, standard deviation(std), minimum(min), first quartile(25%), median(50%), third quartile(75%), and the maximum(max). the essence of this is to have evaluated figures to work with instead of having to work from row to row or column to column.\n",
    "\n",
    "\n",
    "- Next, i stated the various observations i could draw from this estimated data.\n",
    "\n",
    "\n",
    "- I used the isnull() function to check for empty rows and columns but the results were all False(bool) indicating there are no null values in the dataset 'main_list'.\n",
    "\n",
    "\n",
    "- I used the .info() function to to get information about the index, name, non-null countn memory usage and datatype of each column. the 'Non-Null Count' for each column return a values of 1000 further affirming there are no null values or missing entries. I stated my observation as regards null values.\n",
    "\n",
    "\n",
    "### Dealing with datetime feature\n",
    "\n",
    "    it was observed after using the.info() method that the Date and Time columns were not of appropriate datatypes. hence the need for a conversion.\n",
    "    \n",
    "    \n",
    "- the conversion was made using the to_datetime() function on the Date and Time columns using the following format:\n",
    "\n",
    "    *main_list['column name'] = pd.to_datetime(main_list['column_name'])*\n",
    "    \n",
    "\n",
    "- I also checked the Date and Time columns datatypes using thev .info() method which i used earlier on to ensure they were converted succesfully.\n",
    "\n",
    "\n",
    "### Extracting features from date and time\n",
    "\n",
    "- Moving on, i used the following dt.day, dt.month, dt.year, dt.hour methods to extract the days, months, years and hours and assigned them new columns.\n",
    "\n",
    "- After the previous step, i used the .nunique() function on the newly created HOUR column to get the number of unique hours(11). i also used the .unique() function on the same column to get a list of these unique hours.\n",
    "\n",
    "\n",
    "### Getting unique values in columns\n",
    "\n",
    "- I was able to obtain a list of categorical columns in the dataset *main_list* using an iterative approach(for loop) and assigned the list to a new variable *Categorical_columns*.\n",
    "\n",
    "\n",
    "\n",
    "- After getting the categorical columns, the next step taken was to get a the lsits of the unique values of each categorical column using the compounded functions unique().to_list(). next, i printed out the number of these unique values present in each categorial column.\n",
    "\n",
    "\n",
    "- Next, i used the value_counts() method to generate a series containing the counts of the unique values from each categorical column.\n",
    "\n",
    "\n",
    "### Aggregating with GroupBy\n",
    "\n",
    "- The first step here was creating a groupby object eith the City column and also experimented with aggregation functions of sum and mean on the 'Tax 5%' column.\n",
    "\n",
    "\n",
    "- Next was to display a table which shows the gross income of each city and also determine the city with the highest total gross income. This was achived by getting the max. value of the gross income of each city from the group by object 'City_Column'. The result showed that Port Harcourt city has the highest total gross income.\n",
    "\n",
    "\n",
    "- going further, results were obtain for the mean, maximun and sum of the values in 'the Unit prince' column using the aggregation function.\n",
    "\n",
    "\n",
    "### Data Visualization\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Insights\n",
    "Explain the insights you were able to uncover from analysing the datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Future work\n",
    "\n",
    "suggest tasks you might include in future work to make this project more robust\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# StandOut Section\n",
    "Explain what you did differently in the project following the instructions in the notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Executive Summary\n",
    "Include your executive summary document in your repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
